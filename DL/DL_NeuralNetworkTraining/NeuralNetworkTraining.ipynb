{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **신경망 학습**\n",
    "\n",
    "\n",
    "신경망 모델의 매개변수(가중치와 편향)를 데이터로부터 자동으로 학습하는 과정을 말합니다. 이 학습 과정은 주어진 입력 데이터와 해당 데이터에 대한 정답(타깃)을 이용하여 모델의 예측 값과 실제 값 사이의 오차를 최소화하는 방향으로 매개변수를 조정하는 것을 목표로 합니다.\n",
    "\n",
    "신경망 학습 단계:\n",
    "\n",
    "1. 데이터 준비: 학습에 사용할 입력 데이터와 해당 데이터의 정답(타깃)을 준비합니다. 일반적으로 데이터는 훈련 데이터와 검증 데이터, 그리고 시험 데이터로 나뉩니다.\n",
    "\n",
    "2. 초기화: 신경망의 가중치와 편향을 무작위로 초기화합니다. 이 초기화는 모델의 성능과 학습 과정에 영향을 미칩니다.\n",
    "\n",
    "3. 순전파 (Forward Propagation): 초기화된 신경망을 사용하여 입력 데이터를 순방향으로 전달하여 예측 값을 계산합니다.\n",
    "\n",
    "4. 손실 함수 계산: 순전파를 통해 얻은 예측 값과 실제 타깃 값 사이의 오차를 계산하기 위해 손실 함수를 사용합니다. 일반적으로 평균 제곱 오차(Mean Squared Error) 또는 교차 엔트로피 오차(Cross-Entropy Error)와 같은 손실 함수를 사용합니다.\n",
    "\n",
    "5. 역전파 (Backpropagation): 손실 함수의 값을 최소화하기 위해 역전파 알고리즘을 사용하여 오차를 역방향으로 전파하며, 가중치와 편향을 조정합니다.\n",
    "\n",
    "6. 매개변수 업데이트: 역전파를 통해 계산된 그래디언트를 사용하여 최적화 알고리즘(예: 경사 하강법)을 이용하여 가중치와 편향을 업데이트합니다.\n",
    "\n",
    "7. 3~6단계를 반복: 훈련 데이터를 사용하여 여러 에포크(epoch)에 걸쳐 반복적으로 모델을 학습시킵니다.\n",
    "\n",
    "8. 검증 데이터를 사용하여 모델 성능 평가: 학습 과정에서 사용하지 않은 검증 데이터를 사용하여 모델의 성능을 평가하고, 과적합(Overfitting)을 방지하기 위해 조정합니다.\n",
    "\n",
    "9. 시험 데이터를 사용하여 최종 성능 평가: 학습된 모델을 시험 데이터를 사용하여 최종적으로 평가합니다.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "*\"학습: 훈련데이터로부터 가중치 매개변수의 최적값을 자동으로 찾는 것\"*\n",
    "\n",
    "*\"신경망 학습의 목표: 손실함수의 결괏값을 가장 작게만드는 가중치 매개변수를 찾는 것\"*\n",
    "\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-28 at 4.41.51 PM.png\" style=\"width: 700px; height: 250px;\">\n",
    "\n",
    "**\"컴퓨터 비전 분야에서 SIFT,SURF,HOG라는 알고리즘은 이미지의 주요 특징을 추출해, 특징 벡터로 변환하고, 변환된 벡터를 가지고 지도학습 방식의 분류기법인 SVM,KNN등으로 학습 가능\"**\n",
    "\n",
    "> ###### *SIFT(Scale-Invariant Feature Transform): 이미지의 크기와 회전에 불변적인 특징점을 검출하고 특징을 디스크립터(이미지나 객체의 특징을 수치적으로 표현하는 알고리즘)로 표현하는 알고리즘\n",
    "> ###### *SURF(Speeded-Up Robust Features): SIFT와 유사한 크기와 회전에 불변적인 특징점을 빠르게 검출하는 알고리즘\n",
    "> ###### *HOG(Histogram of Oriented Gradients): 이미지의 그래디언트 방향 정보를 히스토그램(데이터의 빈도수를 막대 그래프로 시각화한 것으로, 값의 분포를 나타냄)으로 표현하여 객체의 형태를 나타내는 특징을 추출하는 알고리즘.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **접근 방식의 문제점: 인간의 개입**\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-28 at 5.05.23 PM.png\" style=\"width: 700px; height: 350px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **훈련 데이터와 시험데이터**\n",
    "- *기계학습 문제는 훈련데이터와 시험데이터로 나눠 학습과 실험을 수행*  \n",
    "- *훈련데이터만을 사용해서 최적의 매개변수를 찾고,시험데이터를 사용하여 모델의 실력을 평가*\"\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "##### **기계학습 순서**\n",
    "*훈련 데이터를 학습하며 최적의 매개변수를 찾음 > 시험 데이터를 사용하여 앞서 훈련된 모델의 실력을 평가*\n",
    "\n",
    "*훈련 데이터와 시험 데이터를 나누는 이유 : **범용적으로 사용하기 위함**\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **손실함수**\n",
    "*\"신경망 학습에서 사용되는 하나의 지표로, 최적의 매개변수 값을 탐색하기위해 평균 제곱 오차(MSE), 교차엔트로피오차(CEE)를 사용\"*\n",
    "\n",
    "*\"현재 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하는지를 나타냄\"*\n",
    "\n",
    "#### **평균제곱오차(MSE)**\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-28 at 6.00.31 PM.png\" style=\"width: 500px; height: 150px;\">\n",
    "\n",
    "${k}$ : 데이터 차원 수,\n",
    "${y_k}$ : 신경망의 출력,\n",
    "${t_k}$ : 정답 레이블\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **평균제곱오차(MSE)구현하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"함수 내부에서는 먼저 y와 t의 차이를 제곱한 값 (y-t)^2를 계산하고, np.sum() 함수를 사용하여 이 제곱 오차들의 합을 구합니다. 마지막으로 0.5를 곱하여 평균제곱오차를 계산합니다.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답 '2'\n",
    "\n",
    "# 예1 : '2'일 확률이 높다고 추정(0.6)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 소프트맥스 함수의 출력 \n",
    "mean_squared_error(np.array(y), np.array(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예2: '7'일 확률이 가장 높다고 추정(0.06)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] # 소프트맥스 함수의 출력 \n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **교차 엔트로피 오차(CEE)**\n",
    "\n",
    "*\"교차 엔트로피 오차(Cross-Entropy Error)는 주로 분류 문제에서 사용되는 손실 함수 중 하나이고, 모델의 예측과 실제 타깃(레이블) 값 사이의 차이를 측정하여 모델의 성능을 평가하고 최적화하는 데 활용\"*\n",
    "\n",
    "##### **수식표현의 의미**\n",
    "*​${t_k}$ : 실제값(타겟)으로, 정답 레이블을 나타내는 배열입니다.*\n",
    "\n",
    "*${t_k}$ 배열은 모든 클래스에 대해 0 또는 1의 값을 가지며, 정답 클래스에 해당하는 원소만 1이고 나머지 원소는 0입니다. 이것은 정답 레이블을 원-핫 인코딩 형태로 표현한 것입니다.*\n",
    "\n",
    "*${y_k}$ : 모델의 예측값으로, 모델이 각 클래스에 대해 출력하는 확률 값을 나타내는 배열입니다. 모델의 출력값은 보통 확률로 표현되며, 각 클래스에 대해 0과 1 사이의 값으로 표현됩니다.*\n",
    "\n",
    "*$log({y_k})$: ${y_k}$ 배열의 각 원소에 대해 자연로그를 취합니다. 로그를 취하는 이유는 모델의 예측값과 실제값 간의 오차를 측정하는 지표로서 교차엔트로피를 사용하기 위함입니다.*\n",
    "\n",
    "*${t_k}log({y_k})$: 실제값과 예측값 간의 원소별 곱을 수행합니다. 이것은 오차를 표현하는데 사용됩니다.*\n",
    " \n",
    "*${t_k}$ 배열은 정답 레이블이므로 대응하는 ${y_k}$ 값이 1에 가까울수록 오차가 작아집니다.*\n",
    "\n",
    "*$\\sum_{k} t_k \\log(y_k)$: 모든 클래스에 대해 $t_k \\log(y_k)$ 값을 합산합니다. 이것은 모델의 전체 예측과 실제값 간의 오차를 나타냅니다.*\n",
    "\n",
    "*$-\\sum_{k} t_k \\log(y_k)$: 마지막으로 오차를 음수로 변환하여 교차엔트로피오차를 계산합니다. 음수를 붙이는 이유는 교차엔트로피를 최소화하는 방향으로 모델을 학습하기 위해서입니다.최적화 알고리즘은 일반적으로 손실 함수를 최소화하는 방향으로 모델을 학습하므로, 교차엔트로피오차에 음수를 붙여서 손실 함수를 최소화하는 방향으로 학습하게 됩니다.*\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-29 at 11.33.07 AM.png\" style=\"width: 500px; height: 400px;\">\n",
    "\n",
    "- 확률 곱셈의 덧셈 변환: 자연로그를 취하면 확률 곱셈이 덧셈으로 변환되어 계산이 간편해짐\n",
    "\n",
    "- 수치 안정성: 자연로그를 사용하면 수치 안정성이 향상, 특히 매우 작은 확률 값을 다룰 때에도 로그 변환을 통해 수치적으로 안정적인 계산이 가능\n",
    "\n",
    "- 손실 함수의 미분: 자연로그를 취한 교차 엔트로피 오차의 미분이 간단하고 편리,이를 통해 경사 하강법과 같은 최적화 알고리즘에서 매개변수를 업데이트하는데 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **크로스 엔트로피 오차를 계산하는 파이썬 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*np.log()에 0을 입력하면 -$\\infty$ 가 되어 계산을 진행할 수 없기 때문에, delta를 더해 방지*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **미니배치 학습**\n",
    "\n",
    "*\"미니배치: 데이터를 작은 묶음으로 나누어 처리하는 방식\"*\n",
    "\n",
    "*\"주어진 데이터셋이 빅데이터수준이라면 교차 엔트로피 오차로 손실 함수의 합을 구하려면 많은 시간이 소요되는 문제가 있기 때문에, 데이터 일부를 무작위로 추려 전체의 근사치로 이용하는데, 여기서 그 일부를 미니배치라고 하고, 이러한 학습 방법을 미니배치 학습이라고 함*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 미니 배치학습 구현\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위 추출\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32351,  5767, 43482, 31090, 15269, 40161, 13202, 47391, 20502,\n",
       "       46318])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(배치용)교차 엔트로피 오차 구현하기**\n",
    "*\"배치: 전체 데이터셋으로 한 번에 처리하는 방식\"*\n",
    "\n",
    "\n",
    "배치용 교차 엔트로피 오차를 구현하는 이유: 미니배치(mini-batch) 학습을 위해 전체 데이터셋을 작은 묶음으로 나누어 처리하는 경우에 효율적인 계산을 가능하게 하기 위함\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**장점**\n",
    "\n",
    "- 학습 속도 개선: 전체 데이터셋을 한 번에 처리하는 것보다 미니배치로 처리하면 학습 속도가 향상\n",
    "\n",
    "- 메모리 효율성: 전체 데이터를 한 번에 메모리에 올리는 것보다 미니배치로 처리하면 메모리 효율성이 높아지기 때문\n",
    "\n",
    "- 일반화 성능 향상: 미니배치를 사용하면 데이터셋이 무작위로 섞이기 때문에 모델이 데이터의 순서에 영향을 받지 않게 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **왜 손실 함수를 설정하는가**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **모델의 성능 평가**: 손실 함수는 모델이 얼마나 정확하게 예측하는지를 측정하는 지표로 사용\n",
    "\n",
    "- **최적화를 위한 목표 제공**: 최적화 알고리즘을 사용하여 손실 함수를 최소화하는 방향으로 모델의 가중치와 편향을 업데이트 함. 이때, 손실 함수가 최소가 되는 지점은 모델의 예측이 실제 정답과 가장 가까운 지점이므로, 손실 함수는 모델을 학습하는 데에 목표를 제공\n",
    "\n",
    "- **다중 클래스 분류 문제에서 확률 분포 표현**: 손실 함수는 다중 클래스 분류 문제에서 모델의 출력을 확률 분포로 표현하는 데에 사용됨, 이를 통해 모델이 각 클래스에 속할 확률을 예측하고, 정답과의 차이를 계산\n",
    "\n",
    "*\"신경망 학습에서는 최적의 매개변수(가중치와 편향)를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는데, 이때 매개변수의 미분(기울기)을 계산하고, 그 미분 값을 단서로 매개변수의 값을 갱신하는 과정을 반복\"*\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "##### **정확도를 지표로 삼으면?**\n",
    "*\"신경망을 학습할 때 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문에 정확도를 지표로 삼아서는 안됨\"*  \n",
    "*\"계단함수를 활성화 함수로 사용하지 않듯, 정확도는 매개변수의 미소한 변화에 거의 반응을 보이지 않기 때문에, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화함\"*\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## **수치 미분**\n",
    "\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-31 at 8.10.19 AM.png\" style=\"width: 500px; height: 100px;\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-31 at 8.29.50 AM.png\" style=\"width: 500px; height: 350px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return(f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **편미분**\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-31 at 8.38.19 AM.png\" style=\"width: 500px; height: 100px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편미분 구현\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1**2]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-31 at 8.40.12 AM.png\" style=\"width: 500px; height: 300px;\">\n",
    "\n",
    "## **기울기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        # f(x + h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x - h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        # 중앙 차분으로 수치 미분\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **경사법(경사하강법)**\n",
    "\n",
    "*\"경사법(Gradient Descent)은 기계 학습과 최적화 알고리즘에서 널리 사용되는 최적화 방법 중 하나입니다. 이 방법은 함수의 최솟값(또는 최댓값)을 찾는데 사용됩니다. 특히 머신 러닝에서는 손실 함수(loss function)의 최솟값을 찾기 위해 주로 적용됩니다.\"*\n",
    "\n",
    "*\"경사법은 이름 그대로 함수의 기울기(경사)를 이용하여 최솟값을 찾는 방법입니다. 함수의 기울기는 해당 지점에서 가장 빠르게 증가하는 방향을 가리키며, 최솟값을 찾기 위해서는 기울기가 가리키는 방향의 반대 방향으로 이동하는 것이 핵심 원리입니다.\"*\n",
    "\n",
    "#### **경사법의 작동 방식**\n",
    "\n",
    "1. 초기값을 설정합니다. 이 값은 함수의 최솟값을 찾기 위한 시작점입니다.\n",
    "\n",
    "2. 초기값에서 함수의 기울기(경사)를 계산합니다. 이를 통해 현재 위치에서 함수의 증가 방향을 알 수 있습니다.\n",
    "\n",
    "3. 기울기의 반대 방향으로 조금씩 이동합니다. 이동 거리는 학습률(learning rate)이라고 불리는 하이퍼파라미터로 조절됩니다. 작은 학습률은 안정적이지만 수렴하는 데 오래 걸릴 수 있고, 큰 학습률은 빠르게 수렴하지만 발산할 수 있습니다.\n",
    "\n",
    "4. 새로운 위치에서 다시 기울기를 계산하고, 2번과 3번의 단계를 반복합니다.\n",
    "\n",
    "5. 함수의 기울기가 거의 0에 가까워질 때까지 이동하며, 이 때의 위치가 최솟값으로 추정됩니다.\n",
    "\n",
    "6. 경사법은 다양한 종류가 있으며, 가장 기본적인 형태는 배치 경사 하강법(Batch Gradient Descent)입니다. 또한, 미니 배치 경사 하강법(Mini-batch Gradient Descent)와 확률적 경사 하강법(Stochastic Gradient Descent)과 같은 변형들도 있습니다.\n",
    "\n",
    "##### **수식으로 나타낸 경사법**\n",
    "\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-31 at 9.01.33 AM.png\" style=\"width: 500px; height: 170px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드로 구현한 경사법\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*f: 최적화 하려는 함수  \n",
    "*init_x는 초깃값  \n",
    "*lr: learning rate  \n",
    "*step_num: 경사법에 따른 반복횟수  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **하이퍼파라미터란?**\n",
    "*\"신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의해서 자동으로 획득되는 매개변수인 반면, 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수\"*\n",
    "\n",
    "## **신경망에서의 기울기**\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-31 at 9.37.40 AM.png\" style=\"width: 500px; height: 170px;\">\n",
    "\n",
    "\n",
    "## **학습 알고리즘 구현**\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-31 at 9.45.06 AM.png\" style=\"width: 600px; height: 300px;\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-31 at 10.03.09 AM.png\" style=\"width: 700px; height: 600px;\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-07-31 at 10.14.29 AM.png\" style=\"width: 600px; height: 300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "> # **신경망의 하이퍼파라미터**\n",
    "*\"각 층의 뉴런 수, 배치 크기, 매개변수 갱신 시의 학습률과 가중치 감소 등\"*\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **적절한 하이퍼파라미터 값 찾기**\n",
    "- 시험데이터를 사용하여 하이퍼파라미터를 조정하면 시험 데이터에만 적합하게 조정되어 오버피팅 될 수 있기 때문에 시험데이터 사용x\n",
    "<br>\n",
    "\n",
    "- 하이퍼파라미터를 찾을 땐, 검증데이터를 이용하여 적절한 하이퍼파라미터 값을 찾아야함\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### **훈련 데이터, 검증 데이터, 시험데이터의 용도**\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-08-01 at 3.44.31 PM.png\" style=\"width: 800px; height: px;\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **하이퍼파라미터의 최적화**\n",
    "*\"최적 값이 존재하는 범위를 조금씩 줄여간다\"*\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-08-01 at 3.50.33 PM.png\" style=\"width: 800px; height: ;\">\n",
    "\n",
    "*베이즈 정리를 중심으로 한, 수학 이론을 구사하여 더 엄밀하고 효율적으로 최적화를 수행하는 '베이즈 최적화'라는 방법도 있음*\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-08-01 at 3.58.53 PM.png\" style=\"width: 800px; height: ;\">\n",
    "\n",
    "- 잘될 것 같은 범위를 관찰하고 범위를 좁혀감\n",
    "<br>\n",
    "\n",
    "- 축소된 범위로 똑같은 작업을 반복\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## **주요내용**\n",
    "<img src=\"../DL_NeuralNetworkTraining/assets/img/Screenshot 2023-08-01 at 4.05.22 PM.png\" style=\"width: 800px; height: ;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>*\"매개변수 갱신 방법\"*</u>\n",
    "\n",
    "- **확률적 경사 하강법 (Stochastic Gradient Descent, SGD):**  \n",
    "각 학습 단계에서 무작위로 선택한 데이터 포인트의 기울기를 이용하여 가중치를 업데이트하는 방법으로, 전체 데이터셋을 사용하지 않고 학습 속도를 향상시킴\n",
    "<br>\n",
    "\n",
    "- **모멘텀 (Momentum):**  \n",
    "기울기 업데이트에 이전 업데이트 방향과 크기를 고려하여 가속도를 추가하는 방법으로, 학습 속도를 증가시켜 지역 최소점에서 빠져나오는데 도움을 줌\n",
    "<br>\n",
    "\n",
    "- **AdaGrad (Adaptive Gradient Algorithm):**  \n",
    "각 매개변수에 대해 학습률을 조정하여 희소한 기울기를 강조하는 방법으로, 자주 등장하는 특성에 대해 학습 속도를 감소시킴\n",
    "<br>\n",
    "\n",
    "- **Adam (Adaptive Moment Estimation):**  \n",
    "모멘텀과 AdaGrad의 장점을 결합한 방법으로, 이동 평균을 사용하여 기울기와 제곱 기울기의 비편향 추정치를 계산하고 학습률을 조정하여 효율적인 최적화를 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>*\"가중치 초깃값을 정하는 방법은 어떻게 올바른 학습을 하는 데 매우 중요한가\"*</u>\n",
    "\n",
    "- **기울기 소실 방지:**  \n",
    "신경망의 각 층은 가중치에 의해 연결되어 있으며, 초기 가중치가 너무 작거나 큰 경우, 역전파 과정에서 기울기가 지수적으로 작아지거나 커지는 기울기 소실 또는 폭발 문제가 발생할 수 있습니다. 올바른 초기화는 이러한 문제를 방지하여 학습이 원활하게 진행될 수 있도록 도와줍니다.\n",
    "<br>\n",
    "\n",
    "- **효율적인 학습:**  \n",
    "적절한 가중치 초깃값을 사용하면 초기 학습 단계에서 빠르게 수렴하고 더 높은 정확도로 학습할 수 있습니다. 올바른 초깃값은 초기 학습 속도를 향상시키고, 훈련 데이터에 과적합되는 위험을 줄여줍니다.\n",
    "<br>\n",
    "\n",
    "- **일반화 능력 향상:**  \n",
    "적절한 초깃값은 신경망이 새로운, 이전에 보지 못한 데이터에 대해서도 더 잘 일반화되도록 돕습니다. 이는 모델의 일반화 능력을 향상시켜 새로운 데이터에서도 뛰어난 성능을 보이는 모델을 만드는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>*\"가중치의 초깃값으로 왜 'Xavier 초깃값과', 'He 초깃값'이 효과적인가\"*</u>\n",
    "\n",
    "- **Xavier 초깃값:**  \n",
    "Xavier 초깃값은 시그모이드 함수와 같이 양 끝단이 비선형인 활성화 함수를 사용하는 층에서 효과적. 각 가중치를 평균이 0이고 분산이 1/n인 정규 분포나 -1/n~1/n 사이의 균등 분포로 초기화합니다(n은 뉴런의 개수를 의미). Xavier 초깃값은 층의 입력과 출력의 범위가 적당하게 조절되므로 역전파 시 기울기 소실과 폭발을 막을 수 있습니다.\n",
    "\n",
    "- **He 초깃값:**  \n",
    "He 초깃값은 ReLU(렐루) 활성화 함수를 사용하는 층에서 효과적. 각 가중치를 평균이 0이고 분산이 2/n인 정규 분포나 -√(6/n)~√(6/n) 사이의 균등 분포로 초기화합니다.\n",
    "ReLU 함수는 음수 입력에 대해 0으로 변환하기 때문에 Xavier 초깃값보다 가중치를 더 크게 초기화해야 하는데, He 초깃값은 ReLU 함수의 특성을 잘 반영하여 층의 뉴런들이 쉽게 깨어나고, 학습 과정에서 빠르게 수렴할 수 있도록 도와줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>*\"배치 정규화를 사용하면 어떻게 학습을 빠르게 진행할 수 있으며, 초깃값에 영향을 덜 받게 되는가\"*</u>\n",
    "\n",
    "\n",
    "- **학습 속도 개선:**  \n",
    "배치 정규화는 각 미니배치에서의 입력 데이터를 평균과 분산을 사용하여 정규화하는 방법입니다. 이를 통해 학습 과정에서 입력 데이터 분포를 안정화시키고, 기울기 소실 또는 폭발 문제를 완화합니다. 이러한 안정화는 신경망이 더 빠르게 수렴하고 학습 과정이 안정적으로 진행되도록 돕습니다.\n",
    "\n",
    "- **초깃값에 영향 덜 받음:**  \n",
    "배치 정규화는 가중치 초깃값에 상대적으로 덜 민감합니다. 이는 배치 정규화가 입력 데이터를 정규화하여 각 층의 활성화 값의 분포를 안정화시켜주기 때문입니다. 따라서 가중치 초기화를 더욱 간단하게 설정하고, 초깃값에 크게 의존하지 않아도 됩니다.\n",
    "\n",
    "- **규제 효과:**  \n",
    "배치 정규화는 미니배치마다 평균과 분산을 사용하여 정규화하고, 이를 통해 학습 중에 노드간의 과적합을 방지하는 일종의 규제 효과도 제공합니다. 이로 인해 네트워크의 일반화 성능을 향상시키고, 과적합을 줄이는 데 도움을 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>*\"오버피팅을 억제하는 기술 가중치 감소와 Dropout은 무엇인가\"*</u>\n",
    "\n",
    "- **가중치 감소 (Weight Decay):**  \n",
    "가중치 감소는 학습 과정에서 손실 함수 가중치의 크기에 해당하는 항을 추가하여, 가중치가 커지지 않도록 제한하고, 가중치 값들이 작아지도록 유도하여 모델의 복잡도를 줄여 과적합을 방지하는 정규화 기술\n",
    "\n",
    "- **드롭아웃 (Dropout):**  \n",
    "드롭아웃은 신경망의 복잡성을 줄여 오버피팅을 억제하는 또 다른 정규화 기술입니다. 학습 과정에서 임의로 선택된 뉴런들을 일시적으로 제외시키는 방법을 의미합니다. 특정 확률로 뉴런을 사용하지 않기 때문에, 매 학습 단계에서 다양한 뉴런들이 사용되어 신경망은 더 강건하고 다양한 정보를 학습하도록 유도되며, 과적합을 줄이는 데에 큰 도움을 줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>*\"하이퍼파라미터값 탐색은 왜 최적값이 존재할 법한 범위를 좁혀가며 하는것이 효과적인가\"*</u>\n",
    "\n",
    "- **탐색 효율 향상:**  \n",
    "넓은 범위에서 무작위로 하이퍼파라미터를 탐색하는 것보다, 좁힌 범위에서 시스템적인 탐색을 진행하는 것이 효율적입니다. 탐색 과정을 체계적으로 수행하면 최적 값에 더 빠르게 접근할 수 있습니다.\n",
    "\n",
    "- **Overfitting 방지:**  \n",
    "너무 좁은 범위에서 시작하면 최적 값에 미치지 못할 수 있으며, 너무 큰 범위에서 탐색하면 오버피팅이 발생할 가능성이 있습니다. 점차 범위를 좁힘으로써 적절한 최적 값에 더 가깝게 접근하면서 오버피팅을 방지할 수 있습니다.\n",
    "\n",
    "- **더 정확한 최적 값 도출:**  \n",
    "점차 좁힌 범위에서 탐색하면 최적 값에 더 가까운 후보들을 조사하게 되므로, 보다 정확하고 우수한 최적 값을 도출할 가능성이 높아집니다. 이를 통해 모델의 성능을 최대한 끌어올릴 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
